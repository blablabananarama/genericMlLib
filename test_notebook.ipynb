{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "from autograd.misc.optimizers import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Var:\n",
    "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, value, _children=(), _op=''):\n",
    "        self.value = value\n",
    "        self.grad = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Var) else Var(other)\n",
    "        out = Var(self.value + other.value, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Var) else Var(other)\n",
    "        out = Var(self.value * other.value, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Var(self.value**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.value**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Var(0 if self.value < 0 else self.value, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.value > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        # doesn't work correctly yet\n",
    "        out = Var((1/(1+math.exp(-self.value))), (self,), 'sigmoid')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1/(1+math.exp(-self.value))*(1-(1/(1+math.exp(-self.value))))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(value={self.value}, grad={self.grad})\"\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        # sortable by gradient\n",
    "        return self.grad < other.grad\n",
    "     \n",
    "    \n",
    "x = Var(0.5)\n",
    "y = Var(4.2)\n",
    "z = Var(4.5)\n",
    "i = Var(2.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_graph = []\n",
    "visited = set()   \n",
    "def topo_sort(vertex):\n",
    "    if vertex not in visited:\n",
    "        visited.add(vertex)\n",
    "        for child in vertex._prev:\n",
    "            topo_sort(child)\n",
    "        sorted_graph.append(vertex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2] + [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Unit:\n",
    "    def zero_grad(self):\n",
    "        for p in self.params():\n",
    "            p.grad = 0\n",
    "            \n",
    "    def params(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Unit):\n",
    "    def __init__(self, n_inputs, activation_fun):\n",
    "        # weight initialization\n",
    "        self.weights = [Var(random.uniform(-1,1)) for i in range(n_inputs)]\n",
    "        self.bias = Var(0)\n",
    "        self.activation_fun = activation_fun\n",
    "    \n",
    "    def params(self):\n",
    "        return self.weights + [self.bias]\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        weighted_sum = sum((weight_i * input_i for weight_i, input_i in zip(self.weights, inputs))) + self.bias\n",
    "        activation = weighted_sum.relu() if self.activation_fun == \"relu\" else weighted_sum.sigmoid()\n",
    "        return activation\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"relu Neuron with {len(self.weights)} inputs\"\n",
    "        \n",
    "class Layer(Unit):\n",
    "    def __init__(self, n_neurons, n_inputs, activation_fun=\"relu\"):\n",
    "        self.neurons = [Neuron(n_inputs, activation_fun) for i in range(n_neurons)]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.neurons.__repr__()\n",
    "\n",
    "    def params(self):\n",
    "        return [p for n in self.neurons for p in n.params()]\n",
    "    \n",
    "class NeuralNetwork(Unit):\n",
    "    def __init__(self):\n",
    "        self.layers = [Layer(2,2),Layer(2,2)]\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        result = inputs\n",
    "        for i in self.layers:\n",
    "            result = [j(result) for j in i.neurons]\n",
    "        return result\n",
    "        \n",
    "    def params(self):\n",
    "        return [p for layer in self.layers for p in layer.params()]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \",\".join([i.__repr__() for i in self.layers])\n",
    "        \n",
    "\n",
    "def regularization(alpha, params, norm=\"l2\"):\n",
    "    if norm == \"l2\":\n",
    "        reg_val = Var(sum([param.value**2 for param in params]))\n",
    "    else:\n",
    "        reg_val = Var(math.sqrt(sum([param.value**2 for param in params])))\n",
    "    return reg_val\n",
    "    \n",
    "def loss(prediction, ground_truth):\n",
    "    intermediate = [(i - j)**2 for i, j in zip(prediction, ground_truth)]\n",
    "    temp = Var(0)\n",
    "    for i in intermediate:\n",
    "        temp = temp + i\n",
    "    return temp\n",
    "\n",
    "# def cross_entropy_loss(prediction, ground_truth):\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1947507843544343"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(31)\n",
    "random.gauss(1,.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.9997585332977175, grad: -0.08831431196788514\n",
      "loss: 0.994478621599775, grad: -0.3281391417107534\n",
      "loss: 0.9775121892022117, grad: -0.29557802595666116\n",
      "loss: 0.9533595291415744, grad: -0.2970723945503267\n",
      "loss: 0.9195711543473696, grad: -0.20539193314449083\n",
      "loss: 0.8911686634365154, grad: -0.07507545069264054\n",
      "loss: 0.8707196428239394, grad: -0.02496002976490136\n",
      "loss: 0.8551411283725094, grad: 0.03440135859585691\n",
      "loss: 0.8422056327006071, grad: 0.05085003356671511\n",
      "loss: 0.8316038797853977, grad: 0.062171125838355046\n",
      "loss: 0.8222510170210671, grad: 0.07227800944076432\n",
      "loss: 0.8136189635614076, grad: 0.0744875194963883\n",
      "loss: 0.8055515187372255, grad: 0.045880145566450314\n",
      "loss: 0.7981658799380738, grad: 0.030520339543677943\n",
      "loss: 0.7909969181604317, grad: 0.019815265717903013\n",
      "loss: 0.7839141715923317, grad: 0.007236907340615661\n",
      "loss: 0.7768597134925382, grad: -0.006543597553677408\n",
      "loss: 0.7697939847806642, grad: -0.020991482234627448\n",
      "loss: 0.7626934004766497, grad: -0.03567182996969598\n",
      "loss: 0.7555484649205064, grad: -0.0502250830581851\n",
      "loss: 0.7483620317510689, grad: -0.06435119831461787\n",
      "loss: 0.7411475104619272, grad: -0.0777999296691299\n",
      "loss: 0.7339269376224844, grad: -0.09036530072145266\n",
      "loss: 0.7267289166510696, grad: -0.10188272936997812\n",
      "loss: 0.7195864912039861, grad: -0.1122275645570146\n",
      "loss: 0.7125350539207734, grad: -0.12131405128611539\n",
      "loss: 0.7056116516537702, grad: -0.13435616297449765\n",
      "loss: 0.698872121352648, grad: -0.1406974393479259\n",
      "loss: 0.6923225531353445, grad: -0.14572050210208531\n",
      "loss: 0.6860036803535211, grad: -0.15230956623362027\n",
      "loss: 0.6799368832081217, grad: -0.15465511986072517\n",
      "loss: 0.6741143794110519, grad: -0.14990793882305825\n",
      "loss: 0.668548091849534, grad: -0.1495843869779285\n",
      "loss: 0.6632163121372103, grad: -0.148645255383151\n",
      "loss: 0.658119889721382, grad: -0.14693407276817338\n",
      "loss: 0.6532566809546408, grad: -0.14485241099535864\n",
      "loss: 0.6486273869200705, grad: -0.14238528722261684\n",
      "loss: 0.6442331726886971, grad: -0.13974190730757757\n",
      "loss: 0.6400735815343644, grad: -0.13649142692040744\n",
      "loss: 0.6361348775891295, grad: -0.13303013062207408\n",
      "loss: 0.632411508890206, grad: -0.13252361933797274\n",
      "loss: 0.6289288694235284, grad: -0.12893437835302912\n",
      "loss: 0.6256507041547952, grad: -0.125298093166396\n",
      "loss: 0.6225819131942537, grad: -0.12426173300782593\n",
      "loss: 0.6197202669246988, grad: -0.12040758320469258\n",
      "loss: 0.6170385566287317, grad: -0.1163381721446019\n",
      "loss: 0.6145224922887259, grad: -0.11231986692178882\n",
      "loss: 0.6121652641903641, grad: -0.10838964549624398\n",
      "loss: 0.6099616914151589, grad: -0.10452320370038502\n",
      "loss: 0.6079061286418208, grad: -0.10070822528447254\n",
      "loss: 0.6059923564306664, grad: -0.09690485799923848\n",
      "loss: 0.6042122316819645, grad: -0.0931220511166024\n",
      "loss: 0.6025564963341756, grad: -0.08936333825520155\n",
      "loss: 0.6010128409569997, grad: -0.08566851429477376\n",
      "loss: 0.5995706838042488, grad: -0.0821464897252816\n",
      "loss: 0.5982253830696385, grad: -0.08098661186049659\n",
      "loss: 0.5969748656701725, grad: -0.07772015923411005\n",
      "loss: 0.5958071035668295, grad: -0.0745651981408762\n",
      "loss: 0.5947179613512631, grad: -0.07150873298952015\n",
      "loss: 0.5937033690314069, grad: -0.06854209057336895\n",
      "loss: 0.5927623186572316, grad: -0.0672249701764925\n",
      "loss: 0.5918910565242557, grad: -0.06436801402484528\n",
      "loss: 0.5910783688960612, grad: -0.061592320518526845\n",
      "loss: 0.5903206196665614, grad: -0.05890946620936732\n",
      "loss: 0.5896141173620065, grad: -0.056302604853991696\n",
      "loss: 0.588954786669675, grad: -0.051969927002784584\n",
      "loss: 0.5883400453940762, grad: -0.04963648383341695\n",
      "loss: 0.5877668614969597, grad: -0.04739432427348421\n",
      "loss: 0.5872328224799349, grad: -0.04676359774460879\n",
      "loss: 0.5867355474199132, grad: -0.04316990058105579\n",
      "loss: 0.5862728738696743, grad: -0.04250091931671681\n",
      "loss: 0.5858426201738959, grad: -0.0404839059754915\n",
      "loss: 0.5854427558500943, grad: -0.03853146960927356\n",
      "loss: 0.5850711750560712, grad: -0.036665526749007075\n",
      "loss: 0.5847249065094541, grad: -0.034903135725756804\n",
      "loss: 0.584401098542889, grad: -0.033225677611377374\n",
      "loss: 0.5840985777236986, grad: -0.032361051632969665\n",
      "loss: 0.5838170903126803, grad: -0.0313182361127505\n",
      "loss: 0.5835548261952666, grad: -0.03035535103682385\n",
      "loss: 0.5833098085060222, grad: -0.0287993555845282\n",
      "loss: 0.5830797310698748, grad: -0.027297715910108274\n",
      "loss: 0.5828636245075747, grad: -0.025849507252937422\n",
      "loss: 0.5826604757345198, grad: -0.024453111742775417\n",
      "loss: 0.5824690752763381, grad: -0.023113070834983287\n",
      "loss: 0.5822886327529865, grad: -0.0218283110996169\n",
      "loss: 0.582118205545456, grad: -0.02060057075211048\n",
      "loss: 0.5819567676447347, grad: -0.019436351772403545\n",
      "loss: 0.5818033005147527, grad: -0.0183402224740342\n",
      "loss: 0.5816569063444729, grad: -0.017302656927149357\n",
      "loss: 0.5815171185104829, grad: -0.016317467655010624\n",
      "loss: 0.581383539987575, grad: -0.015379787861549817\n",
      "loss: 0.5812558097072348, grad: -0.014485501030143082\n",
      "loss: 0.5811335959785072, grad: -0.013631112869662523\n",
      "loss: 0.5810165917402168, grad: -0.01281364486208925\n",
      "loss: 0.580904511089749, grad: -0.012030545848378267\n",
      "loss: 0.5807970738965832, grad: -0.011275084486987066\n",
      "loss: 0.5806939167327125, grad: -0.010548010487044632\n",
      "loss: 0.5805947635806635, grad: -0.009852844457533276\n",
      "loss: 0.5804994076379433, grad: -0.009187374311349548\n",
      "loss: 0.5804076556239441, grad: -0.008549681882288778\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#def training(num_epochs, lr = 0.1):\n",
    "num_epochs = 100\n",
    "# generate random data\n",
    "gt = [random.randint(0,1) for i in range(100)]\n",
    "inputs = [[random.gauss(1,.2),random.gauss(1,.2)] if i == 1 \n",
    "          else [random.gauss(0,.2),random.gauss(0,.2)] for i in gt]\n",
    "\n",
    "gt = [[1,0] if i == 1 else [0,1] for i in gt]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.scatter([i[0] for i in inputs], [i[1] for i in inputs])\n",
    "\n",
    "\n",
    "\n",
    "def training(num_epochs, data, g_truth, training_rate = 0.1):\n",
    "    nn = NeuralNetwork()\n",
    "    losses = []\n",
    "    test_loss = []\n",
    "    for i in range(num_epochs):\n",
    "        # create loss\n",
    "        loss_list = [loss(nn.forward(data_i), gt_i) for data_i, gt_i in zip(data[0:70], g_truth[0:70])]\n",
    "        l = sum(loss_list) * (1.0/len(loss_list))\n",
    "        losses.append(l.value)\n",
    "        \n",
    "        # generate gradients\n",
    "        nn.zero_grad()\n",
    "        l.backward()\n",
    "        \n",
    "        # test loss\n",
    "        test_loss_list = [loss(nn.forward(data_i), gt_i) for data_i, gt_i in zip(data[70:], g_truth[70:])]\n",
    "        test_loss.append((sum(test_loss_list) * (1.0/len(test_loss_list))).value)\n",
    "        \n",
    "        grads = 0\n",
    "        #update gradients\n",
    "        for p in nn.params():\n",
    "            grads += p.grad\n",
    "            p.value -= training_rate * p.grad\n",
    "        \n",
    "        print(f\"loss: {l.value}, grad: {grads}\")\n",
    "    return nn, losses, test_loss\n",
    "\n",
    "nn, losses, test_losses = training(num_epochs, inputs, gt)\n",
    "plt.plot(losses, label=\"trainig loss [MSE]\")\n",
    "plt.plot(test_losses, label=\"test loss [MSE]\")\n",
    "plt.legend()\n",
    "plt.savefig(\"/Users/jonas/Documents/ml/genericMlLib/test/loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brew install graphviz\n",
    "# pip install graphviz\n",
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
    "    \n",
    "    for n in nodes:\n",
    "        dot.node(name=str(id(n)), label = \"{ value %.4f | grad %.4f }\" % (n.value, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    \n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librust_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Var' object has no attribute '__att__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bea392518e6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrust_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__att__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Var' object has no attribute '__att__'"
     ]
    }
   ],
   "source": [
    "f = librust_grad.Var(2)\n",
    "g = librust_grad.Var(3)\n",
    "h = f + g\n",
    "h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
